method: bayes
metric:
  name: val/reward
  goal: maximize
run_cap: 100
parameters:
  feedforward_hidden:
    values: [128, 256,]
  num_encoder_layers:
    values: [1, 2, 3,]
  num_decoder_layers:
    values: [1, 2, 3,]
  num_heads:
    values: [4, 8,]
  ppo_epochs:
    values: [3, 5, 7, 10, 15]
  batch_size:
    values: [64, 128, 256] # 32, 64, 128 (each sample is a trajectory)
  mini_batch_size:
    values: [0.125, 0.25, 0.5,] # todo: 1.0 causes issue with sampling
  lr:
    min: 0.000001
    max: 0.001
  feasibility_lambda:
    min: 0.01
    max: 1.0
  entropy_lambda:
    min: 0.01
    max: 1.0
  normalization:
    values: ["batch", "layer"]
  normalize_adv:
    values: [False, True]
  normalize_return:
    values: [False, True]
  tanh_squashing:
    values: [False, True]