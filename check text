\section{MDP of Master Planning Problem} \label{app_drl_unc:MDP}

\subsection{Sets and Parameters} \label{app_drl_unc:sets_params}
Provided the sets and parameters in Section \ref{sec_drl_unc:definitions}, we introduce additional subsets of the transport set $\textit{TR}$ given port $p \in {P}$:
\begin{itemize}
    \item Onboard transports: $\textit{TR}^{\textit{OB}}_p = \{(i,j)\!\in\!{P}^2\!\mid\! i\leq p, j > p\}$
    \item Arrival transports: $\textit{TR}^{\textit{ROB}}_p = \{(i,j)\!\in\!{P}^2\!\mid\! i < p, j > p\}$
    \item Load transports: $\textit{TR}^{{+}}_p = \{(p,j) \!\in\!{P}^2\!\mid\! j > p\}$
    \item Discharge transports: $ \textit{TR}^{{-}}_p = \{(i,p) \!\in\!{P}^2\!\mid\! i < p\}\;$
    \item Transports in crane operations: $\textit{TR}^{M}_p = \textit{TR}^{+}_p \cup \textit{TR}^{-}_p$
\end{itemize}

Considering episode parameters $\zeta$, we define:
\begin{itemize}
    \item Transports: $\textit{tr} = (i,j) \in \textit{TR}$
    \item Cargo types: $k = (\kappa_1, \kappa_2, \kappa_3) \in K$
\end{itemize}

For each combination $(i,j,k)$, we associate the expected demand $\mu^{(i,j,k)}$, standard deviation $\sigma^{(i,j,k)}$, TEU per container $\textit{teu}^{(i,j,k)}$, container weight $w^{(i,j,k)}$, and revenue per container $\textit{rev}^{(i,j,k)}$.

The TEU per container depends on $k$ as:
\[
\textit{teu}(k) =
\begin{cases}
1, & \text{if } \kappa_1 = \text{20 ft.} \\
2, & \text{if } \kappa_1 = \text{40 ft.}
\end{cases} \; .
\]

Similarly, the container weight is defined by:
\[
\textit{w}(k) =
\begin{cases}
1, & \text{if } \kappa_2 = \text{Light} \\
2, & \text{if } \kappa_2 = \text{Medium} \\
3, & \text{if } \kappa_2 = \text{Heavy}
\end{cases} \; .
\]

Both $\textit{teu}$ and $w$ are broadcasted to shape $n_q = |K|\times|\textit{TR}|$ in the MDP formulations for consistency in dimensionality.

The revenue function is given by:
\[\textit{rev}(i, j, k) =
\begin{cases}
(j - i)(1 - \textit{LR}) \!+\! 0.1, & \text{if } \kappa_3 = \text{Long} \\
(j - i) \! + \! 0.1, & \text{if } \kappa_3 = \text{Spot}
\end{cases} \; .
\]

The parameters $\mu$ and $\sigma$ are randomly generated, as shown in Appendix \ref{app_drl_unc:gen}.

\subsection{Formal MDP}
We define the MDP by decomposing the traditional CO problem outlined in \cite{van_twiller_efficient_2024}.

In the traditional MPP, cargo is loaded onto a vessel at each port in a voyage. Let \( u \in \mathbb{R}^{n_u} \) represent the vessel's utilization over the voyage, which is defined by the set of ports \( P = \{1,2,\dots,N_P\} \). Let us also recall the following:
\[
n_u = |B| \times |D| \times |K| \times |\textit{TR}|
\]
\[n_c = |B|\times|D|, \quad n_q = |K|\times |\textit{TR}| \quad n_u = \{n_c \times n_q\}\]

Utilization \( u \) can be decomposed into individual voyage legs, corresponding to the segments between consecutive ports. Specifically, we decompose as:
\[
u = (u_0, u_1, \dots, u_{N_{P-1}})
\]
where each \( u_p \in \mathbb{R}^{n_u} \) represents the vessel's utilization immediately after operations at port \( p \).

\subsubsection{Feasible Region}\label{app_drl_unc:FR_port}
Suppose we have a feasible region for the MPP, where:
\begin{itemize}
    \item \( A' \in \mathbb{R}^{m_u \times n_u} \) is the constraint matrix,
    \item \( b' \in \mathbb{R}^{m_u} \) is the bound vector,
    \item \( u_p \in \mathbb{R}^{n_u}_{\geq 0} \) is the nonnegative vessel utilization.
\end{itemize}
The feasible region, denoted as \( \textit{PH} \), is given by:
\[
\textit{PH}({s}_p) = \{ u_p \in \mathbb{R}^{n_u}_{\geq 0} \mid A' u_p \leq b' \} \;.
\]

At port $p$, utilization can be decomposed into load operations and pre-load utilization:
\[u_p = u'_{p} + u^+_{p} \; ,\]

where:
\begin{itemize}
    \item \( u^+_{p} \in \mathbb{R}^{n_u}_{\geq 0} \) represents load operations,
    \item \( u'_p = u_{p-1} - u^-_{p} \) is the utilization before load operations,
    \item \( u_{p-1} \in \mathbb{R}^{n_u}_{\geq 0} \) is the previous step's utilization,
    \item \( u^-_{p} \in \mathbb{R}^{n_u}_{\geq 0} \) is the discharge operations.
\end{itemize}

Consequently, we can rewrite the feasible region as:
\[
\textit{PH}({s}_p) = \{ u^+_p \in \mathbb{R}^{n_u}_{\geq 0} \mid A' u^+_p \leq b' - A'u'_p \} \;.
\]

\subsection{Decomposed MDP} \label{app_drl_unc:FR_time}
Utilization can be decomposed into sequential steps to refine temporal granularity, thereby obtaining an decomposed MDP formulation. We decompose $u$ as:
\[
u = (u_0, u_1, \dots, u_{T_\textit{seq}})\;,
\]
where \( u_t \in \mathbb{R}^{n_u} \) represents the utilization at time step \( t \), and \( t \in H = \{1,2,\dots,T_\textit{seq}\} \) denotes the episodic horizon \( H \).

Each step $t$ represents a transport and cargo type, as tuple $(\textit{pol}_t, \textit{pod}_t, k_t)$.  Algorithm \ref{alg_drl_unc:mdp_simulation} illustrates an episode of the decomposed MDP. First, we reset the state \( s_0 \), initialize time \( t \), and an empty trajectory. The episode iterates over load ports (\( \textit{pol}_t \)), discharge ports (\( \textit{pod}_t \)), and cargo classes (\( k_t \)). At each step, we sample action  \( x_t \) from policy \( \pi_{\theta}(x|s_t) \) conditioned on state $s_t$ and episode parameters $\zeta$, and transition to state $s_{t+1}$. Afterwards, we store the results in the trajectory and increment time $t$. This process continues until all combinations of \( \textit{pol}_t \), \( \textit{pod}_t \), and \( k_t \) are explored, accumulating a total of \( T_\textit{seq} \) steps.

\begin{algorithm}[h]
\caption{Episode of Augmented MDP}
\label{alg_drl_unc:mdp_simulation}
\begin{algorithmic}[1]
\State \textbf{Require:} $\mathcal{T}$, $\pi_\theta$, $\zeta$, $\mathcal{Q}$
\State $q_{T_\textit{seq}}^{(i,j,k)} \sim \mathcal{Q}(\mu^{(i,j,k)}, \sigma^{(i,j,k)}) \; \forall (i,j) \in \textit{TR}, k \in K$
\State $s_0 \gets (\mathbf{0}^{n_u}, q_{T_\textit{seq}} \odot \mathbf{e}^+_0)$, $t \gets 0$, Trajectory $\gets \{\}$
\For{$\textit{pol}_t = 1$ to $N_P - 1$}
    \For{$\textit{pod}_t = \textit{pol}_t + 1$ to $N_P$}
        \For{$k_t \in K$}
            \State $x_t \sim \pi_\theta(x | s_t, \zeta)$
            \State $s_{t+1} \sim \mathcal{T}(s_t, x_t, \zeta)$
            \State Append $(s_t, x_t, r_t, s_{t+1})$ to Trajectory
            \State $t \gets t + 1$
        \EndFor
    \EndFor
\EndFor
\State \Return Trajectory
\end{algorithmic}
\end{algorithm}

\subsubsection{Transitions}
We use a stochastic transition function \( \mathcal{T}(s_{t+1} | s_t, x_t, \zeta) \in \Delta(S) \). The transition consists of sequential steps:
\begin{enumerate}
     \item If $t \in T_{\text{new port}}$, port demand is revealed. This means we show $q_t^{(i,j,k)} \; \forall (i,j) \in \textit{TR}^+_{\textit{pol}_t}, k\in K$.
    \item If $t \in T_{\text{new port}}$, onboard cargo is discharged \( u_{t+1} = u_t \odot (1 - \mathbf{e}_t^-) \), where \( \mathbf{e}_t^- \in \{0, 1\}^{n_q} \) is a binary mask indicating the cargo type and transport to nullify in \( u_t \).
    \item Each time $t$, cargo is loaded onboard \( u_{t+1} = u_t + x_t \odot \mathbf{e}_t^+ \), where \( \mathbf{e}_t^+ \in \{0, 1\}^{n_q} \) is a binary indicator specifying cargo types and transports to add to \( u_t \).
\end{enumerate}


Additionally, we define the set of time steps before we leave for a new port \( p + 1 \) is defined as follows:
\begin{align*}
T_{\text{leave port}} &= \Big\{ t \in H \ \mid \exists p \in P^{N_P-1}_1 \text{ such that } \\
&\qquad\qquad t = |K|\Bigl(p(N_P\!-1\!) - \frac{p(p-1)}{2}\Bigr) - 1 \Big\}.
\end{align*}

Finally, the set of time steps at which we arrive at a new port \( p \) is defined as follows:
\begin{align*}
T_{\text{new port}} &= \Big\{ t \in H \ \mid \exists p \in P^{N_P-1}_1 \text{ such that } \\
&\qquad\qquad t = |K|\Bigl((p-1)(N_P\!-1\!) - \frac{p(p-1)}{2}\Bigr) \Big\}.
\end{align*}



\subsubsection{Feasible Region}
The state-dependent feasible region for each time $t$ is formulated as:
\[
\textit{PH}(s_t) = \{ u_t \in \mathbb{R}^{n_u}_{\geq 0} \mid A' u_t \leq b' \}
\]

Similar to the port utilization, utilization can be decomposed into load operations and pre-load utilization:
\[u_t = u'_{t} + u^+_{t}\]
where:
\begin{itemize}
    \item \( u^+_{t} \in \mathbb{R}^{n_u}_{\geq 0} \) represents load operations,
    \item \( u'_t = u_{t-1} - u^-_{t} \) is the utilization before load operations,
    \item \( u_{t-1} \in \mathbb{R}^{n_u}_{\geq 0} \) is the previous step's utilization,
    \item \( u^-_{t} \in \mathbb{R}^{n_u}_{\geq 0} \) is the discharge operations.
\end{itemize}

Using the decomposition, we obtain the feasible region as:
\[
\textit{PH}(s_t) = \{ u_t \in \mathbb{R}^{n_u}_{\geq 0} \mid A' (u^+_t + u'_t) \leq b' \}
\]

\subsubsection{Substituting Load Operations for Actions}
Actions \( x_t \) correspond to transformed load operations \( u^+_t \), given by:
\[
x_t = u^+_{t} M(s_t), \quad M(s_t) \in \{0,1\}^{n_u \times n_c},
\]
where \( M(s_t) \) is a state-dependent sparsity mask that selects relevant elements from \( u^+_t \).

However, load operations are subject to \( m_u \) constraints, whereas actions adhere to \( m_c \) constraints. To bridge this difference, we define the state-dependent constraint matrix:
\[
A(s_t) = T(s_t)^\top A' M(s_t), \quad T(s_t) \in \{0,1\}^{m_u \times m_c},
\]
where:
\begin{itemize}
    \item \( A' \) is the original constraint matrix of shape \( (m_u, n_u) \),
    \item \( T(s_t) \) maps the constraints of \( u_t^+ \) to that of \( x_t \)
    \item \( M(s_t) \) maps the space of \( u_t^+ \) to that of \( x_t \),
\end{itemize}

Similarly, we introduce a state-dependent bound:
\[
b''(s_t) =  T(s_t)^\top b' ,
\]
where:
\begin{itemize}
    \item \( b' \) is the original bound of shape \( (m_u, 1) \),
    \item \( T(s_t) \) maps the constraints of \( u_t^+ \) to that of \( x_t \)
\end{itemize}

\subsubsection{Feasible Region for Actions}
Using the refined notation, we express the state-dependent feasible region in terms of actions:
\[
\textit{PH}(s_t) = \{ x_t \in \mathbb{R}^{n_c}_{\geq 0} \mid A(s_t) x_t \leq b''(s_t) - A'u'_t  \}.
\]

Next, we define the updated bound as:
\[
b(s_t) = b''(s_t) - A' u'_t.
\]

Substituting this into the feasible region, we obtain:
\[
\textit{PH}(s_t) = \{ x_t \in \mathbb{R}^{n_c}_{\geq 0} \mid A(s_t) x_t \leq b(s_t) \}.
\]

\subsection{MPP Constraints}\label{app_drl_unc:mpp_constraints}
Let us specify the MPP constraints of $\textit{PH}({s}_p)$ and $\textit{PH}(s_t)$.

\subsubsection{Demand Constraints}
Let us consider the demand subset of $\textit{PH}({s}_p)$ as:
\[
\textit{PH}({s}_p)_\text{dem} = \{ {x}_p \in \mathbb{R}^{n_u}_{\geq 0} \mid A'_\text{dem} {x}_p \leq b'_\text{dem} - A'_\text{dem} u'_p \} \;.
\]

We sum over all vessel locations to obtain an aggregated number of containers of shape $n_q$. Note that only current load actions ${x}_p$ are relevant for $q_p$, hence we can omit $ A'_\text{dem} u'_p $ as pre-loading utilization has already satisfied its demand requirements.
\begin{align*}
{x}_p^\top \mathbf{1}_{n_c} & \leq q_p
\end{align*}

Consider the demand subset of $\textit{PH}(s_t)$ as:
\[
\textit{PH}(s_t)_\text{dem} = \{ x_t \in \mathbb{R}^{n_c}_{\geq 0} \mid A(s_t)_\text{dem} x_t \leq b'(s_t)_\text{dem} - A'_\text{dem}u'_t \} \;.
\]

Right now, we can sum the full vector $x_t$ as it needs to sum to scalar $q_t^{(\textit{pol}_t, \textit{pod}_t, k_t)}$. Again, previous steps are irrelevant to demand, hence we can disregard $A'_\text{dem}u'_t$ to obtain:
\begin{align*}
\mathbf{1}^\top x_t & \leq q_t^{(\textit{pol}_t, \textit{pod}_t, k_t)}
\end{align*}

\subsubsection{Capacity Constraints}
Let us consider the constraint subset of $\textit{PH}({s}_p)$ as:
\[
\textit{PH}({s}_p)_\text{cap} = \{ {x}_p \in \mathbb{R}^{n_u}_{\geq 0} \mid A'_\text{cap} {x}_p \leq b'_\text{cap} - A'_\text{cap} u'_p \} \;.
\]

We sum TEU of all cargo types and transports in ${x}_p$ to obtain TEU use per location with shape $n_c$. The TEU of pre-load utilization is also considered by subtracting it from the vessel capacity, obtaining the following:
\begin{align*}
{x}_p\textit{teu} \leq c - u'_p\textit{teu} \; ,
\end{align*}

Consider the demand subset of $\textit{PH}(s_t)$ as:
\[
\textit{PH}(s_t)_\text{cap} = \{ x_t \in \mathbb{R}^{n_c}_{\geq 0} \mid A(s_t)_\text{cap} x_t \leq b'(s_t)_\text{cap} - A'_\text{cap}u'_t \} \;.
\]

Now, we can do the same trick based on a single scalar $\textit{teu}^{(\textit{pol}_t,\textit{pod}_t, k_t)}$ multiplied with the sum of action $x_t$.
\begin{align*}
\textit{teu}^{(\textit{pol}_t,\textit{pod}_t, k_t)} \mathbf{1}^\top x_t  \leq c - u'_t\textit{teu} \; ,
\end{align*}

\subsubsection{Stability Constraints}
The stability constraints require some algebra to derive for $\textit{PH}({s}_p)$ and $\textit{PH}(s_t)$.

The lcg constraint in its original form is given by:
\begin{align*}
\frac{\mathbf{1}^\top (\textit{lm} \odot u_p)}{\mathbf{1}^\top(w  \odot u_p)} & \leq \overline{\textit{lcg}} \; .
\end{align*}

Applying the utilization decomposition, we can obtain the formulation for $\textit{PH}({s}_p)$:
\begin{align*}
    \mathbf{1}^\top (\textit{lm} \odot u_p)
    &\leq \overline{\textit{lcg}} \mathbf{1}^\top (w  \odot u_p) \\
    \mathbf{1}^\top (\textit{lm} \odot u^+_p)
    + \mathbf{1}^\top (\textit{lm} \odot u'_p)
    &\leq \overline{\textit{lcg}} \mathbf{1}^\top (w  \odot u^+_p)  \\
    &\quad + \overline{\textit{lcg}} \mathbf{1}^\top (w  \odot u'_p) \\
    \mathbf{1}^\top (\textit{lm}_p \odot {x}_p)
    - \overline{\textit{lcg}} \mathbf{1}^\top (w \odot {x}_p)
    &\leq \overline{\textit{lcg}} \mathbf{1}^\top (w  \odot u'_p)  \\
    &\quad - \mathbf{1}^\top (\textit{lm} \odot u'_p) \\
    \;\mathbf{1}^\top \! \big( (\textit{lm}\!-\!\overline{\textit{lcg}} w)\!\odot\!{x}_p \big) & \leq \! \; \mathbf{1}^\top\!\big( (\overline{\textit{lcg}} w\!-\!\textit{lm})\!\odot\!u'_p \big) \;.
\end{align*}

This approach extends to both the lower and upper bounds for the lcg and vcg, ensuring that vessel stability is properly maintained at every step.

Based on the $\textit{PH}({s}_p)$ constraint, we can substitute load operations for actions and obtain the formulation for $\textit{PH}(s_t)$:
\begin{align*}
    \;\mathbf{1}^\top \! \big( (\textit{lm}(t) \!-\!\overline{\textit{lcg}} w(t))\!\odot\!x_t \big) & \leq \! \; \mathbf{1}^\top\!\big( (\overline{\textit{lcg}} w\!-\!\textit{lm})\!\odot\!u'_p \big) \;.
\end{align*}

where $w(t) = w^{(\textit{pol}_t,\textit{pod}_t, k_t)}$ and $\textit{lm}(t) = \textit{lm}^{(\textit{pol}_t,\textit{pod}_t, k_t)}$


\subsection{Auxiliary Variables} \label{app_drl_unc:aux_vars}
The reward function contains two auxiliary variables derived from state $s$,  which incur costs due to inefficient port operations. At port $p$, Equation \eqref{for_drl_unc:hm} creates an indicator of hatch movements $\textit{hm}(s, p) \in \{0,1\}^{|B|}$, whereas Equation \eqref{for_drl_unc:ho} computes the number of on-deck containers during hatch movements, causing hatch overstowage $\textit{ho}(s,p) \in \mathbb{R}^{|B|}_{\geq 0}$.
\begin{align}
\textit{hm}(s, p) & = \Bigg(\sum_{k \in K} \sum_{\textit{tr} \in \textit{TR}^M_{p}} u_t^{(b,d^\textit{below},k,\textit{tr})} > 0 \Bigg) \label{for_drl_unc:hm} \\
\textit{ho}(s, p) & = \textit{hm}(s, p) \Bigg(\sum_{k \in K} \sum_{\textit{tr} \in \textit{TR}^\textit{ROB}_{p}} u_t^{(b,d^\textit{above},k,\textit{tr})} \Bigg) \label{for_drl_unc:ho}
\end{align}

Equation \eqref{for_drl_unc:crane_target} computes the target crane moves at port $p$ by equally spreading the total demand per port over pairs of adjacent bays, where $\delta^\textit{cm}$ is the allowed deviation from the equal spread set by ports. Subsequently, Equation \eqref{for_drl_unc:cm} computes the excess crane moves $\textit{cm}(s,p) \in \mathbb{R}^{|B|-1}_{\geq 0}$
\begin{align}
\overline{\textit{cm}}(s, p) & = (1 + \delta^\textit{cm}) \frac{2}{|{B}|} \sum_{\textit{tr} \in \textit{TR}^M_{p}} \sum_{k \in {K}} q^{(\textit{tr},k)}_t \label{for_drl_unc:crane_target} \\
{\textit{cm}}(s, p)  & = \max\Bigg(\sum_{d \in D}\sum_{k \in K}\sum_{\textit{tr} \in \textit{TR}^M_{p}} u_t^{(0:|B|-1,d,k,\textit{tr})} + u_t^{(1:|B|,d,k,\textit{tr})} - \overline{\textit{cm}}(s, p), 0\Bigg)\label{for_drl_unc:cm}
\end{align}


\section{Feasibility Mechanisms} \label{app_drl_unc:feas_proj}
Table \ref{tab_drl_unc:feas_implement} provides an overview of implemented feasibility mechanisms.
\begin{table}[h!]
    \centering
    \small
    \caption{Feasibility mechanisms and relation to constraints}
    \label{tab_drl_unc:feas_implement}
    \begin{tabular}{lll}
        \toprule
        \textbf{Type} & \textbf{Implementation} & \textbf{Constraints} \\
        \midrule
        FR  & Composite loss & Constraints $\textit{PH}(s_t)$\\
        VP  & $\textit{VP}(x_t, A(s_t), b(s_t), \alpha_v, \delta_v)$ & Constraints $\textit{PH}(s_t)$\\
        WS  & $\mathcal{W}(x_t, q_t)$ & Demand $q_t$\\
        PC  & $\mathcal{C}(x_t, 0, c- u'_t \textit{teu})$ & TEU capacity $c$\\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Log Probability Adjustments} \label{app_drl_unc:log_prob}
This subsection provides technical details on the adjustments made to the log-probability distribution of the policy as a result of non-linear transformations to distribution samples.

Projecting actions alters the policy's probability density, necessitating consideration of the {change of variables principle} \cite{bishop_pattern_2006}. This principle ensures valid volume scaling by requiring the transformation $f(x)$ to satisfy:
\begin{enumerate}
    \item \textbf{Differentiability}: $f(x)$ must be differentiable to compute the Jacobian $J_f(x)$ and determine local volume scaling.
    \item \textbf{Non-Singularity}: The Jacobian determinant must be non-zero (\( \det(J_f(x)) \neq 0 \)) to prevent dimensional collapse.
    \item \textbf{Invertibility}: \( f(x) \) must be locally or globally invertible to ensure a one-to-one mapping between points in the original and transformed spaces.
\end{enumerate}

These properties ensure the transformation is smooth, one-to-one, and well-behaved, enabling the use of the Jacobian adjustment \( \log \pi'(x|s) = \log \pi(x|s) - \log |\det (J_f(x))| \) as a valid probability scaling factor.

\subsubsection{Weighted Scaling Projection Layer.}
Suppose we have variable $x \in \mathbb{R}^n_{>0}$ and scalar $y \in \mathbb{R}_{>0}$ and the following piecewise linear function:
\begin{align*}
\mathcal{P}(x, y) =
\begin{cases}
x & \text{if } \mathbf{1}^\top{x} \leq y \\
\frac{x}{\mathbf{1}^\top x} \cdot y & \text{if } \mathbf{1}^\top
{x} > y
\end{cases}
\end{align*}

{Case 1: $\mathbf{1}^\top{x} \leq y$}.
\begin{align*}
\mathcal{P}(x, y) & =  x \\
\frac{\partial}{\partial x} \mathcal{P}(x, y) & =  \frac{\partial x}{\partial x} \\
J_\mathcal{P}(x, y) & =  I_n \\
\end{align*}

{Case 2: $\mathbf{1}^\top{x} > y$}, where we apply the product rule and then the quotient rule of differentiation.
\begin{align*}
\mathcal{P}(x, y) & =  \frac{x}{\mathbf{1}^\top x} \cdot y \\
\frac{\partial}{\partial x} \mathcal{P}(x, y) & =  \frac{\partial}{\partial x} \left(\frac{x}{\mathbf{1}^\top x} \cdot y\right) \\
\frac{\partial}{\partial x} \mathcal{P}(x, y) & =  \frac{\partial}{\partial x} \left(\frac{x}{\mathbf{1}^\top x} \right) \cdot y \\
J_\mathcal{P}(x, y) & =  y \cdot\frac{1}{(\mathbf{1}^\top x)^2} \left(I_n \mathbf{1}^\top x - x \cdot \mathbf{1}^\top  \right)  \\
J_\mathcal{P}(x, y) & =  \frac{y \cdot I_n}{(\mathbf{1}^\top x)} - \frac{y \cdot x^\top }{(\mathbf{1}^\top x)^2}
\end{align*}

We obtain the following Jacobian of function $\mathcal{P}(x, y)$:
\begin{align*}
J_\mathcal{P}(x, y) =
\begin{cases}
I_n & \text{if } \mathbf{1}^\top{x} \leq y \\
\frac{y}{(\mathbf{1}^\top x)^2} \left(I_n\mathbf{1}^\top x - x 1^\top \right)
& \text{if } \mathbf{1}^\top{x} > y
\end{cases}
\end{align*}

Finally, we verify that Jacobian adjustment is allowed for the weighted scaling projection by: \\
\begin{enumerate}
    \item \( \mathcal{P}(x, y)\) has been shown to be differentiable.
    \item Provided that $x,y > 0$, either case is positive definite as the diagonal elements are strictly positive. We obtain $\det(J_\mathcal{P}(x, y)) > 0$, thus the Jacobian in non-singular.
    \item \( \mathcal{P}(x, y) \) is locally invertible as \( \det(J_\mathcal{P}(x, y)) \neq 0 \).
\end{enumerate}

\subsubsection{Violation Projection Layer.}
Suppose we have function $\mathcal{P}(x, A, b) = x - \eta_v A^\top(Ax - b)_{>0}$ with $x \in \mathbb{R}^n_{>0}$, $\eta_v \in \mathbb{R}_{>0}$, $A \in \mathbb{R}_{\geq 0}^{m\times n}$, $b \in \mathbb{R}^m_{\geq 0}$, and $m > n$. \\

Case 1: $\eta_v A^\top(Ax - b) = 0$.
\begin{align*}
\mathcal{P}(x, A, b) & = x \\
\frac{\partial}{\partial x}\mathcal{P}(x, A, b) & = \frac{\partial x}{\partial x} \\
J_\mathcal{P}(x, A, b) & = I_n
\end{align*}

Case 2: $\eta_v A^\top(Ax - b) > 0$, where we apply the chain rule on the second term.
\begin{align*}
\mathcal{P}(x, A, b) & = x - \eta_v A^\top(Ax - b) \\
\frac{\partial}{\partial x}\mathcal{P}(x, A, b) & = \frac{\partial}{\partial x}\left(x - \eta_v A^\top(Ax - b) \right) \\
J_\mathcal{P}(x, A, b) & = I_n -  \eta_v A^\top A
\end{align*}

Both cases are combined in the following matrix formulation with diagonal matrix $\text{Diag} = \text{diag}((Ax - b) > 0)$:
\begin{align*}
J_\mathcal{P}(x, A, b) & = I_n -  \eta_v A^\top \text{Diag} A
\end{align*}

Finally, we can confirm that Jacobian adjustment is allowed for the violation projection layer by:
\begin{enumerate}
    \item $\mathcal{P}(x, A, b)$ is differentiable as shown above.
    \item Due to the full rank nature of the identity $I_n$ and $A^\top \text{Diag} A$ when $\text{Diag} = I_m$, we get $\det(J_\mathcal{P}(x, A, b)) \neq 0$, and hence the Jacobian is non-singular.
    \item $\mathcal{P}(x, A, b)$ is locally invertible as $\det(J_\mathcal{P}(x,A,b)) \neq 0$. It is not globally invertible, due to the piece-wise nature of $(Ax - b)_{>0}$.
\end{enumerate}

\subsubsection{Policy Clipping}
We can implement a clipped Gaussian distribution that enforces element-wise bounds on a standard Gaussian \cite{fujita_clipped_2018}. Let $ \mu_\theta $ and $ \sigma^2_\theta $ denote the policy’s mean and variance, with bounds $ \textit{lb}_\textit{pc} $ and $ \textit{ub}_\textit{pc} $, and $ \Phi(\cdot) $ being the cumulative distribution function of the standard Gaussian. Actions are sampled from $ \mathcal{N}(\mu_\theta, \sigma^2_\theta) $ and clipping the result to $[\textit{lb}_\textit{pc}, \textit{ub}_\textit{pc}]$. Provided this transformation, we compute the log probabilities $ \log \pi(x|s) $ for action $ x $ by:
\begin{align*}
\log \pi(x|s) =
\begin{cases}
\log \Phi\left(\frac{\textit{lb}_\textit{pc} - \mu_\theta}{\sigma_\theta}\right) & \text{if } x \leq \textit{lb}_\textit{pc}, \\
-\frac{(x - \mu_\theta)^2}{2\sigma^2_\theta} - \log(\sqrt{2 \pi \sigma^2_\theta}) & \text{if } \textit{lb}_\textit{pc} < x < \textit{ub}_\textit{pc}, \\
\log \left(1 - \Phi\left(\frac{\textit{ub}_\textit{pc} - \mu_\theta}{\sigma_\theta}\right) \right) & \text{if } x \geq \textit{ub}_\textit{pc}
\end{cases}
\end{align*}

\subsection{Violation Projection Layer}
We define a feasible region of action \( x \) as the polyhedron:
\[
\textit{PH} = \{x \in \mathbb{R}^n : Ax \leq b, x \geq 0\},
\]
where $A \in \mathbb{R}^{m\times n}$ and $b \in \mathbb{R}^{m}$.

Constraints in \( \textit{PH} \) may be violated during optimization. To quantify these violations, we introduce the violation function:
\[
\mathcal{V}(x) = (Ax - b)_{>0},
\]
where \( \mathcal{V}(x)_{m_i} > 0 \) indicates that constraint \( {m_i} \) is violated, and \( \mathcal{V}(x)_{m_i} = 0 \) means the constraint is satisfied.

\textbf{Violation Gradient Descent.} To minimize constraint violations, we update \( x \) for a fixed number of iterations using gradient descent on the violation term \( \|\mathcal{V}(x)\|_2^2 \), which represents the squared distance to feasibility. Differentiating with respect to \( x \), we derive the update rule:
\begin{align*}
    x' &= x - \eta_v \nabla_x \|\mathcal{V}(x)\|_2^2  \\
       &= x - \eta_v 2 A^\top \mathcal{V}(x).
\end{align*}
Since the step size \( \eta_v \in (0,1) \) is a tunable parameter, we simplify the update function to:
\[
x' = x - \eta_v A^\top \mathcal{V}(x).
\]

\begin{theorem}[Convergence of Violation Gradient Descent]
Let \( x_0 \in \mathbb{R}^n \) be an initial point, and consider update:
\[
x_{k+1} = x_k - \eta_v A^\top \mathcal{V}(x_k),
\]

where:
\begin{itemize}
    \item \( \mathcal{V}(x) = \max(0, Ax - b) \) is the element-wise function onto nonnegative constraint values.
    \item \( \eta_v \in (0,1) \) is a step size parameter.
    \item \( A \in \mathbb{R}^{m \times n} \) has full row rank.
    \item The feasible region \( \textit{PH} = \{x \in \mathbb{R}^n : Ax \leq b, x \geq 0\} \) is nonempty.
\end{itemize}

Then, the sequence \( \{x_k\} \) satisfies:
\begin{enumerate}
    \item The function \( g(x) = \|\mathcal{V}(x)\|_2^2 \) is non-increasing.
    \item \( x_k \) converges to a feasible point \( x^* \) or a local minimum violation point.
\end{enumerate}
\end{theorem}

\begin{proof}
Define the violation function:
\[
g(x) = \|\mathcal{V}(x)\|_2^2.
\]

Since \( \mathcal{V}(x) \) is an elementwise projection onto nonnegative values, and some function $h(y) = \max(0,y)$ is convex and non-decreasing, then the function \( g(x) = \|\mathcal{V}(x)\|_2^2 \) is convex when \( Ax - b \) is affine.

We apply gradient descent on \( g(x) \) using the update rule:
\[
x_{k+1} = x_k - \eta_v \nabla_x g(x_k).
\]

By the standard descent lemma \cite{boyd_convex_2004}, for a sufficiently small step size \( \eta_v \), we have:
\[
g(x_{k+1}) \leq g(x_k) - \eta_v \|\nabla_x g(x_k)\|_2^2.
\]

Since \( \eta_v > 0 \) and \( \|\nabla_x g(x_k)\|_2^2 \geq 0 \), it follows that:
\[
g(x_{k+1}) \leq g(x_k).
\]

Thus, \( g(x_k) \) is non-increasing. \\

Since \( g(x_k) \) is also lower-bounded by \( 0 \), it must converge to some limit \( g^* \geq 0 \). This implies:
\[
\lim_{k \to \infty} \|\mathcal{V}(x_k)\|_2 = \Tilde{c}, \quad \text{for some } \Tilde{c} \geq 0.
\]


If \( \Tilde{c} = 0 \), then \( x_k \) converges to a feasible point, meaning \( \mathcal{V}(x_k) = 0 \). If \( \Tilde{c} > 0 \), then \( x_k \) converges to a local minimum of \( g(x) \), where no further descent is possible, satisfying \(\nabla_x g(x_k) = 0, \) which implies \(A^\top \mathcal{V}(x_k) = 0.\)
\end{proof}

\section{Instance Generator} \label{app_drl_unc:gen}
During training, we simulate problem instances based on a Gaussian distribution with element $i$:
\[
q^{(i,j,k)} \sim \mathcal{N}\big(\mu^{(i,j,k)}, \sigma^{(i,j,k)}\big) \; \forall (i,j) \in \textit{TR}, k \in K.
\]
Here, \(\mu\) is the expected value of cargo demand, initialized by a uniform generator \(\mathcal{U}(\textit{lb}, \textit{ub})\), while the standard deviation of demand is defined as \(\sigma^{(i,j,k)} = \textit{CV} \cdot \mu^{(i,j,k)}\), where the coefficient of variation (CV) controls the spread of each element based on \(\mu^{(i,j,k)}\). Note that CV is normally defined as
\(\textit{CV}^{(i,j,k)} = \frac{\sigma^{(i,j,k)}}{\mu^{(i,j,k)}}\), so we use \(\sigma^{(i,j,k)} = \textit{CV} \cdot \mu^{(i,j,k)}\) to control the spread of the distribution.

We initialize $\mu^{(i,j,k)} \sim \mathcal{U}(0,\overline{\mu^{(i,j,k)}}) $, where the upper bound on the expected value is found as follows:
\[
\overline{\mu} = \frac{2 \textit{UR} \cdot \mathbf{1}^\top c}{\textit{NC}} \qquad,
\]
where $\textit{UR}$ is the rate of total utilization present in the demand (e.g., 1.2 means total demand is 120\% of total capacity), and $\textit{NC} \in \mathbb{R}^{|TR|}_{>0}$ is a matrix to spread the demand over different elements proportional to the number of transports remaining to be loaded.

During generalization testing, we simulate problem instances based on a continuous uniform generator:
\[q^{(i,j,k)} \sim \mathcal{U}(\textit{lb}^{(i,j,k)}, \textit{ub}^{(i,j,k)}).\]

To ensure similar mean and variance of the instances, we derive parameters $\textit{lb}$ and $\textit{ub}$ from the definition of the continuous uniform distribution, as follows:
\begin{align*}
\mu^{(i,j,k)} = (\textit{lb}^{(i,j,k)} + \textit{ub}^{(i,j,k)}) / 2 \\
(\sigma^{(i,j,k)})^2 = (\textit{ub}^{(i,j,k)} - \textit{lb}^{(i,j,k)})^2 / 12
\end{align*}

We rewrite to:
\begin{align*}
\textit{lb}^{(i,j,k)} = \mu^{(i,j,k)} - \sqrt{(12 (\sigma^{(i,j,k)})^2)/2}\\
\textit{ub}^{(i,j,k)} = \mu^{(i,j,k)} + \sqrt{(12 (\sigma^{(i,j,k)})^2)/2}
\end{align*}


\section{Multi-Stage Stochastic MIP} \label{app_drl_unc:smip}
\subsection{Multi-Stage Scenario Tree}
A scenario tree is a directed tree represented as \(T_\textit{ST} = (V_\textit{ST}, E_\textit{ST})\), where \( V_\textit{ST} \) is the set of nodes, each corresponding to a decision or uncertainty realization at a given stage. \( E_\textit{ST} \subseteq V_\textit{ST} \times V_\textit{ST} \) is the set of directed edges representing transitions between nodes over time.

The tree consists of:
\begin{enumerate}
    \item A root node \( v_1 \in V_\textit{ST} \), representing the initial state at the first port.
    \item Stages \( p = 1, 2, \dots, N_{P}-1 \), where each node \( v \) belongs to a stage \( p(v) \). We denote stages by $p$, as stages are equivalent to ports in a voyage.
    \item Branching structure, where each node has child nodes that correspond to possible future realizations.
    \item A probability measure \(  P_\textit{ST}: V_\textit{ST} \to [0,1] \) assigning probabilities to nodes, ensuring:     \[
   \sum_{v' \in \text{child}(v)} \mathbb{P}(v') =  \mathbb{P}(v), \quad \forall v \in V_\textit{ST}.
   \]
   \item Scenario paths $\phi \in \mathcal{Z}$, which are root-to-leaf paths representing possible realizations of uncertainty over time.
\end{enumerate}

\subsection{MIP Formulation}
We define the MPP under demand uncertainty as a multi-stage stochastic MIP.

\textbf{Decision Variables.} The following variables are included:
\begin{itemize}
    \item Vessel utilization: $\Tilde{u}^{b,d,\phi}_{\textit{tr},k} \in \mathbb{Z}_{\geq0}$
    \item Hatch overstowage: $\Tilde{\textit{ho}}^{\phi}_{p,b} \in \mathbb{Z}_{\geq0}$
    \item Makespan of cranes: $\Tilde{\textit{cm}}^{\phi}_{p} \in \mathbb{Z}_{\geq0}$
    \item Hatch movement: $\Tilde{\textit{hm}}^{\phi}_{p,b} \in \{0,1\}$
\end{itemize}

All integer constraints are relaxed linearly in the implementation. Additionally, we use a sufficiently large constant, denoted by big $M$, to impose logical constraints as needed.

\textbf{Objective.} The objective function \eqref{for_drl_unc:MIP_obj} maximizes the revenue with parameter \(\textit{rev}^{(i,j,k)} \in \mathbb{R}_{>0}\) and minimizes hatch-overstowage with parameter \( \textit{ct}^\textit{ho} \in \mathbb{R}_{>0} \) and crane moves costs with parameter \( \textit{ct}^\textit{cm} \in \mathbb{R}_{>0} \) over scenario paths $\phi \in \mathcal{Z}$ each with probability $\mathbb{P}_\phi$. We assume each scenario path has equal probability.

\textbf{Constraints.} Constraint \eqref{for_drl_unc:MIP_demand} enforces that the onboard utilization cannot exceed the cargo demand $q$, whereas Constraint \eqref{for_drl_unc:MIP_capacity} limits each vessel location to the TEU capacity $c$ for each bay $b \in B$ and deck $d \in D$. In Constraint \eqref{for_drl_unc:hatch}, we indicate that hatches need to be opened if below deck cargo needs to be loaded or discharged. Based on these movements, Constraint \eqref{for_drl_unc:hatch_restow} models the amount of hatch overstowage in containers. Subsequently, we compute the target of crane moves $\overline{z}$ in Constraint \eqref{for_drl_unc:z_upper}, after which Constraint \eqref{for_drl_unc:long_crane} computes the excess number of crane moves $\Tilde{\textit{cm}}$.

Additionally, we model the longitudinal and vertical stability in Constraints \eqref{for_drl_unc:lm} until \eqref{for_drl_unc:VS1}. First, we compute the longitudinal moment, vertical moment and total weight in Constraints \eqref{for_drl_unc:lm}, \eqref{for_drl_unc:vm} and \eqref{for_drl_unc:TW}, respectively. Second, Constraint \eqref{for_drl_unc:LS1} bounds \textit{lcg} between $\underline{\textit{lcg}}$ and $\overline{\textit{lcg}}$. Third,  Constraint \eqref{for_drl_unc:VS1} bounds \textit{vcg} between $\underline{\textit{vcg}}$ and $\overline{\textit{vcg}}$. Both \textit{lcg} and \textit{vcg} are linearized equivalents of the original Constraints \eqref{for_drl_unc:lcg} and \eqref{for_drl_unc:vcg}, respectively. Furthermore, we include non-anticipation in Constraint \eqref{for_drl_unc:nonanti} to prevent leveraging future demand realizations.
\begin{align}
\text{max } &
\sum_{\phi \in \mathcal{Z}} \mathbb{P}_\phi \sum_{p\in {P}} \sum_{b \in{B}}\sum_{d\in {D}} \sum_{k\in {K}}\sum_{\textit{tr} \in \textit{TR}^\textit{+}(p)} \textit{rev}^{(i,j,k)} \Tilde{u}^{b,d,\phi}_{\textit{tr},k} \nonumber \\
& \qquad\qquad\qquad\qquad\qquad - \textit{ct}^\textit{ho} \Tilde{\textit{ho}}^{\phi}_{p,b} - \textit{ct}^\textit{cm} \Tilde{\textit{cm}}^{\phi}_{p}  \label{for_drl_unc:MIP_obj}
\end{align}
\begin{align}
    \text{s.t. } &
    \sum_{b \in {B}} \sum_{d \in {D}} \Tilde{u}^{b,d,\phi}_{\textit{tr},k} \leq q^{\phi}_{\textit{tr},k} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; \textit{tr} \in \textit{TR}^\textit{OB}(p),\; k \in {K},\; \phi \in \mathcal{Z} \label{for_drl_unc:MIP_demand} \\
    & \sum_{k \in {K}} \sum_{\textit{tr} \in \textit{TR}^\textit{OB}(p)} \textit{teu}_{\textit{tr},k} \Tilde{u}^{b,d,\phi}_{\textit{tr},k} \leq c_{b,d} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; b \in{B},\; d \in {D},\; \phi \in \mathcal{Z} \label{for_drl_unc:MIP_capacity} \\
    & \sum_{k \in {K}} \sum_{\textit{tr} \in \textit{TR}^{M}(p)} \Tilde{u}^{b,d_h,\phi}_{\textit{tr},k} \leq M \Tilde{\textit{hm}}^{\phi}_{p,b} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; b \in{B},\; \phi \in \mathcal{Z} \label{for_drl_unc:hatch} \\
    & \sum_{k \in {K}} \sum_{\textit{tr} \in \textit{TR}^\textit{ROB}(p)} \Tilde{u}^{b,d_o,\phi}_{\textit{tr},k} - M(1 - \Tilde{\textit{hm}}^{\phi}_{p,b}) \leq \Tilde{\textit{ho}}^{\phi}_{p,b} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; b \in{B},\; \phi \in \mathcal{Z} \label{for_drl_unc:hatch_restow} \\
    & {\overline{z}}^{\phi}_{p} = (1 + \delta^\textit{cm}) \frac{2}{|{B}|} \sum_{\textit{tr} \in \textit{TR}^M(p)} \sum_{k \in {K}} q^{\phi}_{\textit{tr},k} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; \phi \in \mathcal{Z} \label{for_drl_unc:z_upper} \\
    & \sum_{b \in{b'}} \sum_{d \in {D}} \sum_{k \in {K}} \sum_{\textit{tr} \in \textit{TR}^{M}(p)} \Tilde{u}^{b,d,\phi}_{\textit{tr},k} - {\overline{z}}^{\phi}_{p} \leq \Tilde{\textit{cm}}^{\phi}_{p} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; {b'} \in {B}',\; \phi \in \mathcal{Z} \label{for_drl_unc:long_crane} \\
    & \textit{tw}^{\phi}_{p} = \sum_{k \in {K}} w_k \sum_{\textit{tr} \in \textit{TR}^\textit{OB}(p)} \sum_{d \in {D}} \sum_{b \in{B}} \Tilde{u}^{b,d,\phi}_{\textit{tr},k} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; \phi \in \mathcal{Z} \label{for_drl_unc:TW} \\
    & \textit{lm}^{\phi}_{p} = \sum_{b \in{B}} \textit{ld}_b \sum_{k \in {K}} w_k \sum_{\textit{tr} \in \textit{TR}^\textit{OB}(p)} \sum_{d \in {D}} \Tilde{u}^{b,d,\phi}_{\textit{tr},k} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; \phi \in \mathcal{Z} \label{for_drl_unc:lm} \\
    & \textit{vm}^{\phi}_{p} = \sum_{d \in {D}} \textit{vd}_d \sum_{k \in {K}} w_k \sum_{\textit{tr} \in \textit{TR}^\textit{OB}(p)} \sum_{b \in{B}} \Tilde{u}^{b,d,\phi}_{\textit{tr},k} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; \phi \in \mathcal{Z} \label{for_drl_unc:vm} \\
    & \underline{\textit{lcg}} \textit{tw}^{\phi}_{p} \leq \textit{lm}^{\phi}_{p} \leq \overline{\textit{lcg}} \textit{tw}^{\phi}_{p} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; \phi \in \mathcal{Z} \label{for_drl_unc:LS1} \\
    & \underline{\textit{vcg}} \textit{tw}^{\phi}_{p} \leq \textit{vm}^{\phi}_{p} \leq \overline{\textit{vcg}} \textit{tw}^{\phi}_{p} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P},\; \phi \in \mathcal{Z} \label{for_drl_unc:VS1} \\
    & \Tilde{u}^{b,d,\phi'}_{\textit{tr},k} = \Tilde{u}^{b,d,\phi}_{\textit{tr},k} \nonumber\\
    & \qquad\qquad\qquad \forall p \in {P}, \textit{tr} \in \textit{TR}^{+}(p), k \in {K},\nonumber\\
    & \qquad\qquad\qquad b \in{B}, d \in {D}, \phi, \phi' \in \mathcal{Z} \mid q^{\phi}_{[p-1]} = q^{\phi'}_{[p-1]} \label{for_drl_unc:nonanti}
\end{align}



\section{Deep RL Implementation Details} \label{app_drl_unc:DRL}
\subsection{PPO Algorithm}
PPO is an on-policy reinforcement learning algorithm that seeks to maximize expected cumulative reward while enforcing stable policy updates via clipped importance sampling \cite{schulman_proximal_2017}, as outlined in Algorithm \ref{alg_drl_unc:ppo}. The agent collects trajectories, computing \(n_\text{ppo}\)-step return to evaluate performance with \( V_{\theta}(s) \) as estimated state value:
\begin{equation}
G_t^{(n_\text{ppo})} = \sum_{k_\text{ppo}=0}^{n_\text{ppo}-1} \gamma^k r_{t+k_\text{ppo}} + \gamma^{n_\text{ppo}} V_{\theta}(s_{t+n_\text{ppo}}),
\label{for_drl_unc:n_return}
\end{equation}

To reduce variance, we adopt Generalized Advantage Estimation (GAE) \cite{schulman_high-dimensional_2016}:
\begin{align}
\hat{A}_t^{\text{GAE}} &= \sum_{l_\text{ppo}=0}^{\infty} (\gamma \lambda)^{l_\text{ppo}} \delta_{t+l_\text{ppo}}, \label{for_drl_unc:gae1} \\
\delta_t &= r_t + \gamma V_{\theta}(s_{t+1}) - V_{\theta}(s_t). \label{for_drl_unc:gae2}
\end{align}
Here, \( \delta_t \) is the temporal difference (TD) residual, which quantifies the advantage of taking action \( x_t \) at state \( s_t \).

The actor is updated using the PPO clipped surrogate loss:
\begin{align}
\mathcal{L}_{\text{actor}}(\theta) &= \mathbb{E}_t \Big[ \min \big( \text{ratio}_t(\theta) \hat{A}_t^{\text{GAE}}, \notag \\
& \quad \text{clip}(\text{ratio}_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t^\text{GAE} \big) \Big], \label{for_drl_unc:ppo_actor}
\end{align}
where the probability ratio is defined as:
\begin{equation}
\text{ratio}_t(\theta) = \frac{\pi_{\theta}(x_t | s_t)}{\pi_{\theta_{\text{old}}}(x_t | s_t)}.
\end{equation}

The critic aims to minimize the squared TD error:
\begin{equation}
\mathcal{L}_{\text{critic}}(\theta) = \mathbb{E}_t \Big[ \big( V_{\theta}(s_t) - G_t^{(n_\text{ppo})} \big)^2 \Big].
\label{for_drl_unc:ppo_critic}
\end{equation}

Finally, the total PPO objective, including feasibility regularization from Equation \eqref{for_drl_unc:feas_loss}, is given by:
\begin{align}
\mathcal{L}(\theta) &= \mathcal{L}_{\text{actor}}(\theta) + \lambda_c \mathcal{L}_{\text{critic}}(\theta)  + \lambda_f \mathcal{L}_\text{feas}(\theta) \notag \\ & \qquad - \lambda_e \mathbb{E}_t\big[ \text{entropy}(\pi_{\theta}) \big],
\label{for_drl_unc:ppo_total_loss}
\end{align}
where \( \lambda_c \), \( \lambda_f \), and \( \lambda_e \) are weighting coefficients for the critic loss, feasibility regularization, and entropy regularization respectively.

\begin{algorithm}
\caption{Proximal Policy Optimization (PPO)}
\label{alg_drl_unc:ppo}
\begin{algorithmic}[1]
\State \textbf{Require:} Model parameters $\theta$, steps $n$, learning rate $\eta$
\For{each gradient update}
    \For{each step $t$}
        \State Collect $n$-step trajectories $\{(s_t, x_t, r_t, s_{t+1})\}$
        \State Compute $n$-step returns $G_t^{(n_\text{ppo})}$
        \State Compute advantage estimates $\hat{A}_t^{\text{GAE}}$
    \EndFor
    \State Update parameters: $\theta \gets \theta + \eta \nabla_{\theta} \mathcal{L}(\theta)$
\EndFor
\State \Return Policy $\pi_\theta$
\end{algorithmic}
\end{algorithm}

\subsection{SAC Algorithm}
Soft Actor-Critic (SAC) is an off-policy reinforcement learning algorithm that optimizes both reward maximization and entropy to encourage efficient exploration \cite{haarnoja_soft_2018}, as outlined in Algorithm \ref{alg_drl_unc:sac}. It is based on maximum entropy reinforcement learning, which aims to learn a stochastic policy that not only maximizes cumulative rewards but also maintains high entropy for robustness and stability. SAC leverages a soft Q-learning approach, using two Q-functions to mitigate overestimation bias, an entropy-regularized policy update, and an automatically adjusted temperature parameter to balance exploration and exploitation.

The algorithm maintains an actor network for policy learning, two Q-function critics for value estimation, a target Q-network for stable learning, and an adaptive temperature parameter to regulate entropy. The loss functions for standard SAC are derived from the Bellman backup equation and the policy gradient formulation, ensuring convergence to an optimal stochastic policy. We also include feasibility regularization from Equation \eqref{for_drl_unc:feas_loss} in the actor loss.
\begin{itemize}
 \item Compute target Q-value:
\end{itemize}
\begin{align*}
Q_\text{target}(s_t,x_t) &= r_t + \gamma \mathbb{E}_{s_{t+1}, x_{t+1} \sim \pi} \Big[ \\
& \min_{l=1,2} Q_{\theta}^l(s_{t+1}, x_{t+1}) - \alpha \log \pi_{\theta}(x_{t+1} | s_{t+1}) \Big]
\end{align*}
\begin{itemize}
    \item Critic loss:
    \[
    \mathcal{L}_\text{critic}(\theta) = \mathbb{E} \Big[ (Q_{\theta}(s_t, x_t) - Q_\text{target}(s_t,x_t))^2 \Big]
    \]
    \item Actor loss:
    \[
    \mathcal{L}_\text{actor}(\theta) = \mathbb{E} \Big[ \alpha \log \pi_\theta(x_t | s_t) - Q_{\theta}(s_t, x_t) + \lambda_f \mathcal{L}_\text{feas}(\theta)  \Big]
    \]
    \item Temperature loss:
    \[
    \mathcal{L}_\alpha(\theta) = \mathbb{E} \Big[ -\alpha (\log \pi_\theta(x_t | s_t) + \text{entropy}_{\text{target}}) \Big]
    \]

\end{itemize}

This formulation ensures stability and encourages exploration by adapting the trade-off between exploitation and exploration dynamically.

\begin{algorithm}[h]
\caption{Soft Actor-Critic (SAC)}
\label{alg_drl_unc:sac}
\begin{algorithmic}[1]
\State \textbf{Require:} Parameters: actor \(\theta_\text{actor}\), critics \(\theta_{\text{critic}}^1, \theta_{\text{critic}}^2\), targets \((\theta_{\text{target}}^1, \theta_{\text{target}}^2) = (\theta_{\text{critic}}^1, \theta_{\text{critic}}^2)\), temperature \(\alpha\), learning rate actor $\eta_a$, learning rate critic $\eta_c$, learning rate temperature $\eta_\alpha$, soft update parameter $\tau$, replay buffer \(\mathcal{D}\).
\For{each iteration}
\For{each environment step $t$}
    \State Sample action \(x_t \sim \pi_\theta(x_t | s_t)\)
    \State Perform transition \(s_{t+1} \sim \mathcal{T}(s_{t+1}|s_t,x_t)\)
    \State Observe reward \(r_t = \mathcal{R}(s_t,x_t)\),
    \State Store \((s_t, x_t, r_t, s_{t+1})\) in \(\mathcal{D}\).
\EndFor
\For{each gradient step}
    \State Sample a minibatch \((s_t, x_t, r_t, s_{t+1})\) from \(\mathcal{D}\).
    \State Compute target Q-value: $Q_\text{target}(s_t,x_t)$
    \State Update parameters: \\
    \quad \(\theta_{\text{critic}}^l \gets \theta_{\text{critic}}^l - \eta_c \nabla_l \mathcal{L}_\text{critic}(\theta) \text{ for } l \in \{1,2\}\)\\
    \quad \(\theta_{\text{actor}} \gets \theta_{\text{actor}} - \eta_a \nabla \mathcal{L}_\text{actor}(\theta)\) \\
    \quad \(\alpha \gets \alpha - \eta_\alpha \nabla \mathcal{L}_\alpha(\theta)\) \\
    \quad \(\theta_{\text{target}}^l \leftarrow \tau \theta_{\text{critic}}^l + (1 - \tau) \theta_{\text{target}}^l \text{ for } l \in \{1,2\}\)
\EndFor
\EndFor
\State \Return Policy $\pi_\theta$
\end{algorithmic}
\end{algorithm}


\subsection{Hyperparameters}
The parameters of the MPP environment are shown in Table \ref{tab_drl_unc:env_params}.

\begin{table}[h!]
    \centering
    \small
    \caption{Environment parameters} \label{tab_drl_unc:env_params}
    \begin{tabular}{lll}
        \toprule
        \textbf{Parameters} & \textbf{Symbol} & \textbf{Value} \\
        \midrule
        Voyage length & $N_P$ & 4 \\
        Number of bays & $N_B$ & 10 \\
        Cardinality deck set & $|D|$ & 2 \\
        Cardinality cargo set & $|K|$ & 12 \\
        Cardinality transport set & $|\textit{TR}|$ & 6 \\
        Vessel TEU & $\mathbf{1}^\top c$ & 1,000 \\
        Long term contract reduction & $\textit{LR}$ & 0.3 \\
        Utilization rate demand & $\textit{UR}$ & 1.1 \\
        lcg bounds & $(\underline{\textit{lcg}},\overline{\textit{lcg}})$ & (0.85,1.05) \\
        vcg bounds & $(\underline{\textit{vcg}},\overline{\textit{vcg}})$ & (0.95,1.15) \\
        Crane moves allowance & $\delta^\textit{cm}$ & 0.25 \\
        Overstowage costs & $\textit{ct}^\textit{ho}$ & 0.33 \\
        Crane move costs & $\textit{ct}^\textit{ho}$ & 0.5 \\
        \bottomrule
    \end{tabular}
\end{table}

Table \ref{tab_drl_unc:ppo_sac_hyperparameters} provides the hyperparameters of projected and vanilla PPO and SAC.

\begin{table*}[t]
    \centering
    \small
    \caption{Hyperparameters for projected and vanilla PPO and SAC} \label{tab_drl_unc:ppo_sac_hyperparameters}
    \begin{tabular}{llcccc}
        \toprule
        \multicolumn{2}{c}{\textbf{Settings}} & \multicolumn{2}{c}{\textbf{Projection Algorithms}}  & \multicolumn{2}{c}{\textbf{Vanilla Algorithms}} \\
        \cmidrule(lr){1-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6}
        \textbf{Hyperparameters} & \textbf{Symbol} & \textbf{PPO} & \textbf{SAC} & \textbf{PPO} & \textbf{SAC} \\
        \midrule
        \textbf{Actor Network} & & Attention & Attention  & Attention  & Attention \\
        \textbf{Number of Heads} & & 8 & 8  & 4  & 4 \\
        \textbf{Hidden Layer Size} & & 128 & 128 & 256  & 256 \\
        \textbf{Encoder Layers} & & 3 & 3  & 2  & 1 \\
        \textbf{Decoder Layers} & & 3 & 3  & 3  & 3 \\
        \textbf{Critic Network} & & $1\times \text{MLP}$ & $2\times \text{MLP}$ & $1\times \text{MLP}$ & $2\times \text{MLP}$ \\
        \textbf{Critic Layers} & & 4 & 4  & 4  & 4 \\
        \textbf{Target Network} & & No & Soft Update & No & Soft Update  \\
        \textbf{Target Update Rate} & $\tau$ & N/A & 0.005 & N/A & 0.005 \\
        \textbf{Dropout Rate} & & $0.009$ & $0.009$  & $0.073$  & $0.164$ \\
        \textbf{Max Policy Std.} & & $1.931$ & $1.931$  & $1.118$  & $1.779$ \\
        \midrule
        \textbf{Optimizer} & & Adam & Adam & Adam & Adam \\
        \textbf{Learning Rate} & $\eta$ & $2.04 \times 10^{-4}$ & $2.04 \times 10^{-4}$ & $9.64 \times 10^{-4}$ & $9.10 \times 10^{-4}$ \\
        \textbf{Batch Size} & & 64 & 64 & 64 & 64 \\
        \textbf{Embedding Size} & & 128 & 128 & 128 & 128 \\
        \textbf{Discount Factor} & $\gamma$ & 0.99 & 0.99 & 0.99 & 0.99 \\
        \textbf{GAE} & $\lambda$ & 0.95 & N/A & 0.95 & N/A \\
        \textbf{Value Coefficient} & $\lambda_c$ & $0.50$  & N/A & $0.50$ & N/A \\
        \textbf{Entropy Coefficient} & $\lambda_e$ & $0.010$  & Learned & $0.061$ & Learned \\
        \textbf{Feasibility Penalty} & $\lambda_f$ & $0.0677$  & $0.0677$ & $0.302$ & $0.065$ \\
        \textbf{Clip Parameter} & $\epsilon$ & 0.2 & N/A & 0.2 & N/A \\
        \textbf{Replay Buffer} & & No & Yes & No & Yes \\
        \textbf{Replay Buffer Size} & & N/A & $10^4$ & N/A & $10^4$ \\
        \textbf{Mini-batch Size} & & 16 & 16 & 32 & 16 \\
        \textbf{Update Epochs} & & 5 & 1 & 1 & 1 \\
        \textbf{Entropy Target} & & N/A & $-|{X}|$ & N/A & $-|{X}|$\\
        \midrule
        \textbf{Projection Learning Rate} & $\eta_v$ & $0.05$ & $0.05$ & N/A & N/A\\
        \textbf{Projection Epochs} & & $100$ & $100$ & N/A & N/A\\
        \textbf{Inference Projection Stop} & $\delta_v$ & $0.05$ & $0.05$ & N/A & N/A\\
        \midrule
        \textbf{Training Budget} & & $7.2 \times 10^{7}$ & $7.2 \times 10^{7}$ & $7.2 \times 10^{7}$ & $7.2 \times 10^{7}$\\
        \textbf{Validation Budget} & & $5.0 \times 10^{3}$ & $5.0 \times 10^{3}$ & $5.0 \times 10^{3}$ & $5.0 \times 10^{3}$\\
        \textbf{Validation Frequency} & & Every 20\% & Every 20\%  & Every 20\%  & Every 20\% \\
        \bottomrule
    \end{tabular}
\end{table*}



\subsection{Additional Experiments} \label{app_drl_unc:add_exp}
In Table \ref{tab_drl_unc:fr}, we analyze different configurations of feasibility regularization (FR). First, we evaluate performance under the same hyperparameters as AM-P policies. Second, we examine the effect of significantly increasing $\lambda_f$. Third, we assess performance with specific hyperparameter tuning, including $\lambda_f$. These experiments indicate that FR can reduce the distance to the feasible region, however, achieving fully feasible instances remains a challenge.


\begin{table*}[t]
\centering
\small
\caption[Evaluation of tuning feasibility regularization hyperparameter]{Performance evaluation on $N$ instances of feasibility regularization (FR) with hyperparameter (H.P.) settings: projected hyperparameters (Proj.), ensuring a fair comparison with projection-based policies, and tuned hyperparameter (Tune), optimized specifically for PPO and SAC with FR. While we use $\lambda_f$ as the control parameter for FR, tuning involves adjusting a Lagrangian multiplier for each constraint. Average performance metrics include objective value in profit (Ob.), inference time in seconds (Time), percentage of feasible instances (F.I.), and total absolute distance to the feasible region $d(\textit{PH}(s_t))$. Note that {\dag} indicates infeasible objectives.}
\label{tab_drl_unc:fr}
\begin{tabular}{lllllrrrr}
\toprule
\multicolumn{5}{c}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{Testing ($\boldsymbol{N=30}$)}} \\
\cmidrule(r){1-5} \cmidrule(r){6-9}
\textbf{Alg.} & \textbf{Model} & \textbf{F.M.} & \textbf{H.P.} & \textbf{$\lambda_f$} & \textbf{Ob. (\$)} & \textbf{Time (s)} & \textbf{F.I. (\%)} & \textbf{$d(\textit{PH}(s_t))$} \\
\midrule
SAC & AM & FR & Proj. & 0.0677 & 1139.78\textsuperscript{\dag} & 13.82 & 0.00 & 62.77 \\
SAC & AM & FR & Proj. & 0.677 & 1031.55\textsuperscript{\dag} & 13.45 & 0.00 & 120.74\\
SAC & AM & FR & Tune  & 0.065 & 1113.03\textsuperscript{\dag} & 12.63 & 0.00 & 37.55\\
\midrule
PPO & AM & FR & Proj. & 0.0677 & 2606.87\textsuperscript{\dag} &  13.36 & 0.00 & 3171.86\\
PPO & AM & FR & Proj. & 0.677 & 2593.29\textsuperscript{\dag}  & 13.20 & 0.00 & 3381.59\\
PPO & AM & FR & Tune  & 0.302 &  1842.46\textsuperscript{\dag} & 11.74 & 0.00 & 754.21\\ \bottomrule
\end{tabular}
\end{table*}