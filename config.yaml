# Environment settings
env:
  ports: 4 # [4, 5, 6]
  bays: 10 # [10, 20,]
  decks: 2
  cargo_classes: 6
  weight_classes: 3
  customer_classes: 2
  capacity: [50] # [50, 500]
  TEU: 1000  # 1000, 20000
  LCG_target: 0.95
  VCG_target: 1.05
  stability_difference: 0.10
  CI_target: 1.25
  hatch_overstowage_costs: 0.333333
  hatch_overstowage_mask: False
  long_crane_costs: 0.5
  episode_order: "standard" # greedy_revenue, priority_then_greedy, max_distance_then_priority, standard
  normalize_obs: True

  # Generator
  seed: 42
  utilization_rate_initial_demand: 1.1
  spot_percentage: 0.3
  iid_demand: True
  cv_demand: 0.5
  demand_uncertainty: True
  generalization: False
  non_anticipation: False

# Model settings
model:
  lr_end_factor: 0.5
  batch_size: 128
  init_dim: 8
  embed_dim: 128
  hidden_dim: 256
  num_encoder_layers: 1
  num_decoder_layers: 3
  num_heads: 4
  dropout_rate: 0.04207531712356412
  normalization: "layer" # layer, batch
  out_bias_pointer_attn: False
  temperature: 1.0
  critic_temperature: 1.0
  tanh_clipping: 0
  tanh_squashing: False
  scale_max: 1.25
  train_decode_type: "continuous_sampling"
  val_decode_type: "continuous_projection"
  test_decode_type: "continuous_projection"
  phase: "tuned_training" # train, test, tuned_training
  encoder_type: "attention" # mlp, attention
  decoder_type: "attention" # mlp, attention
  demand_aggregation: "full" # sum, self_attn, full
  logger: "wandb" #"wandb" # None, wandb

# PPO parameters
algorithm:
  type: "ppo" # ppo, ddpg, sac
  clip_range: 0.2
  ppo_epochs: 7 # 2, 3, 5, 7, 10
  mini_batch_size: 0.25
  normalize_adv: False
  normalize_return: False
  max_grad_norm: 0.5
  tau: 0.005
  kl_threshold: None #0.01 - 0.1
  kl_penalty_lambda: None # 1.0
  gamma: 0.99
  gae_lambda: 0.95
  vf_lambda: 0.5
  entropy_lambda: 0.001967669837772243
  feasibility_lambda: 0.9632909195956464
  projection_lambda: 0.01
  adaptive_feasibility_lambda: False
  lagrangian_multiplier_0: 1.0
  lagrangian_multiplier_1: 1.0
  lagrangian_multiplier_2: 1.0
  lagrangian_multiplier_3: 1.0
  lagrangian_multiplier_4: 1.0
  lagrangian_multiplier_5: 1.0
  lagrangian_multiplier_6: 1.0
  lagrangian_multiplier_7: 1.0
  lagrangian_multiplier_8: 1.0
  lagrangian_multiplier_9: 1.0
  lagrangian_multiplier_10: 1.0
  lagrangian_multiplier_11: 1.0
  lagrangian_multiplier_12: 1.0
  lagrangian_multiplier_13: 1.0
  lagrangian_multiplier_14: 1.0
  lagrangian_multiplier_15: 1.0
  lagrangian_multiplier_16: 1.0
  lagrangian_multiplier_17: 1.0
  lagrangian_multiplier_18: 1.0
  lagrangian_multiplier_19: 1.0
  lagrangian_multiplier_20: 1.0
  lagrangian_multiplier_21: 1.0
  lagrangian_multiplier_22: 1.0
  lagrangian_multiplier_23: 1.0
  lagrangian_multiplier_24: 1.0


# AM PPO parameters
training:
  lr: 0.00020407622212291503
  projection_type: "weighted_scaling_policy_clipping" #"linear_violation", "weighted_scaling_policy_clipping", "None", "weighted_scaling", "policy_clipping"
  #todo: not implemented logprob adaptations - convex_program, linear_program
  projection_kwargs:
    alpha: 0.05
    delta: 0.05
    max_iter: 100
    project_per_port: False
  optimizer: "Adam"   #AdamW, Adam
  validation_freq: 0.1
  validation_patience: 2
  train_data_size: 72_000_000
  val_data_size: 5000
  test_data_size: 5000

testing:
  timestamp: "20250128_135057"
  num_episodes: 30